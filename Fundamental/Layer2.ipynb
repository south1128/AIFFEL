{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Layer2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Sparse Representation\n",
        "A method of directly mapping a word or meaning to a specific dimension of a vector\n",
        "\n",
        "> Let's say Apple is labeled 1 and Banana is labeled 2 <br>\n",
        "Apple : [0,0]<br>\n",
        "Banana : [1,1]<br>\n",
        "Pear : [0,1]<br>\n",
        "pear has similar shape to apple and similar color to banana\n",
        "\n",
        "Shortcomings :<br>\n",
        "There are almost countless meaning categories in the world. you can't really put every own meaning to each dimension of the word vector.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oQzVqIF5qqSW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distributed Representation\n",
        "\n",
        "#### distribution hypothesis\n",
        "Word that appear in a similar context are similar in meaning.\n",
        "\n",
        "#### distributed representation\n",
        "The distance between the two word vectors that appear in a similar context is closer to one another , and words that do not are adjusted little by little so that they are far away. \n",
        "\n",
        "- *specific dimension of vector does not contain specific meaning. The meaning is distributed across several dimensions of the vector.*\n",
        "- *able to calculate similarity between words*"
      ],
      "metadata": {
        "id": "TxikqfjSsJBc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding Layer\n",
        "Basically vocabulary for computer \n",
        "\n",
        "The Embedding layer connects the word that come in as input to a distributed expressions.\n",
        "\n",
        "THEN HOW DOES IT MAP THE WORDS?<br>\n",
        "*By using One-hot Encoding*\n",
        "> One-hot Encoding : <br>\n",
        "- Expressing N words as N-dimensional vectors\n",
        "- Put 1 in the place where the word is included and 0 in the rest.\n"
      ],
      "metadata": {
        "id": "3Ha_BBdZuQwA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRJ4KRuSqnqp",
        "outputId": "2124e0b9-db68-40dc-9cd6-9960680c5189"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "vocab = {\n",
        "    'i' : 0,\n",
        "    'need' : 1,\n",
        "    'some' : 2,\n",
        "    'more' : 3,\n",
        "    'coffee' : 4,\n",
        "    'cake' : 5,\n",
        "    'cat' : 6,\n",
        "    'dog' : 7\n",
        "}\n",
        "\n",
        "sentence = 'i i i i need some more coffee coffee coffee'\n",
        "\n",
        "_input = [vocab[w] for w in sentence.split()]\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "one_hot = tf.one_hot(_input,vocab_size)\n",
        "print(one_hot.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "distribution_size = 2\n",
        "linear = tf.keras.layers.Dense(units=distribution_size,use_bias=False)\n",
        "one_hot_linear = linear(one_hot)\n",
        "\n",
        "print('Linear Weight')\n",
        "print(linear.weights[0].numpy())\n",
        "\n",
        "print('\\nOne-Hot Linear Result')\n",
        "print(one_hot_linear.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygGjj5qBwaaA",
        "outputId": "f1fb175b-e837-425d-dcec-e059ba087098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Weight\n",
            "[[-0.22614408 -0.14133441]\n",
            " [ 0.6897342   0.5778433 ]\n",
            " [-0.47542423  0.43041813]\n",
            " [ 0.5307485  -0.20240974]\n",
            " [-0.5897074  -0.02290857]\n",
            " [ 0.20062464  0.65808415]\n",
            " [ 0.2632308  -0.3739692 ]\n",
            " [ 0.11422312  0.02952564]]\n",
            "\n",
            "One-Hot Linear Result\n",
            "[[-0.22614408 -0.14133441]\n",
            " [-0.22614408 -0.14133441]\n",
            " [-0.22614408 -0.14133441]\n",
            " [-0.22614408 -0.14133441]\n",
            " [ 0.6897342   0.5778433 ]\n",
            " [-0.47542423  0.43041813]\n",
            " [ 0.5307485  -0.20240974]\n",
            " [-0.5897074  -0.02290857]\n",
            " [-0.5897074  -0.02290857]\n",
            " [-0.5897074  -0.02290857]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "some_words = tf.constant([3,57,35])\n",
        "\n",
        "print('Sentence for Embedding : ',some_words.shape)\n",
        "embedding_layer = tf.keras.layers.Embedding(input_dim=64,output_dim=100)\n",
        "\n",
        "print('Embedded Sentence : ',embedding_layer(some_words).shape)\n",
        "print('Weight Form of Embedding Layer : ',embedding_layer.weights[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dp3ua9fDxOr_",
        "outputId": "83b8d932-cadb-4e9f-9273-cc5382c39f28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence for Embedding :  (3,)\n",
            "Embedded Sentence :  (3, 100)\n",
            "Weight Form of Embedding Layer :  <tf.Variable 'embedding/embeddings:0' shape=(64, 100) dtype=float32, numpy=\n",
            "array([[ 0.01321033, -0.01810903,  0.00808107, ...,  0.00741605,\n",
            "        -0.01477399, -0.00947008],\n",
            "       [-0.03704268,  0.0153797 ,  0.03202203, ..., -0.01356131,\n",
            "        -0.01437421, -0.03563211],\n",
            "       [ 0.01879281, -0.00787201,  0.00132377, ..., -0.02735884,\n",
            "         0.01027121,  0.01781802],\n",
            "       ...,\n",
            "       [ 0.04857221, -0.03118016,  0.03480772, ...,  0.01560559,\n",
            "        -0.03280371,  0.0484472 ],\n",
            "       [ 0.04397647, -0.02406415, -0.04433781, ...,  0.04927639,\n",
            "         0.04118277, -0.04138882],\n",
            "       [-0.03885026,  0.02158064,  0.02357776, ...,  0.02342123,\n",
            "        -0.00356811, -0.03133275]], dtype=float32)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because the Embedding layer only corresponds to words, differentiation is impossible.\n",
        "Therfore, it is impossible to connect any result to the embedding layer.\n",
        "\n",
        "*The Embedding layer should be used  directly connected to the input.*"
      ],
      "metadata": {
        "id": "HyMDSlrfyAVH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Sequential\n",
        "- Just because there's no connection between the lists of data, it doesn't mean that it's not sequential data.\n",
        "- But in deep learning sequential data must have the connections between the list of data.\n"
      ],
      "metadata": {
        "id": "KECIvUTZyuO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN\n",
        "Model that processes Sequential data\n",
        "Using only one Weight parameter and sequentially updata corresponding to dimension of `(input dimension ,output dimension)`\n",
        "\n",
        "*problems of RNN*\n",
        "- Vanishing Gradient : The front part of the input becomes lighter as it goes back, causes the loss.\n",
        " "
      ],
      "metadata": {
        "id": "Li6Ojt-BYllV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'What time is it ?'\n",
        "dic = {\n",
        "    'is' : 0,\n",
        "    'it' : 1,\n",
        "    'What' : 2,\n",
        "    'time' : 3,\n",
        "    '?' : 4\n",
        "}\n",
        "\n",
        "print('Sentence for RNN',sentence)\n",
        "\n",
        "sentence_tensor = tf.constant([[dic[word] for word in sentence.split()]])\n",
        "\n",
        "print('word mapping for Embedding : ',sentence_tensor.numpy())\n",
        "print('Form of input data : ',sentence_tensor.shape)\n",
        "\n",
        "embedding_layer = tf.keras.layers.Embedding(input_dim=len(dic),output_dim=100)\n",
        "emb_out = embedding_layer(sentence_tensor)\n",
        "\n",
        "print('\\nEmbedding Result : ',emb_out.shape)\n",
        "print('Weight Form of Embedding layer : ',embedding_layer.weights[0].shape)\n",
        "\n",
        "rnn_seq_layer = \\\n",
        "tf.keras.layers.SimpleRNN(units=64, return_sequences=True,use_bias=False)\n",
        "rnn_seq_out = rnn_seq_layer(emb_out)\n",
        "\n",
        "print('\\nRNN Result : ',rnn_seq_out.shape)\n",
        "print('Weight Form of RNN layer : ',rnn_seq_layer.weights[0].shape)\n",
        "\n",
        "rnn_fin_layer = tf.keras.layers.SimpleRNN(units=64,use_bias=False)\n",
        "rnn_fin_out = rnn_fin_layer(emb_out)\n",
        "\n",
        "print('\\n RNN Result : ',rnn_fin_out.shape)\n",
        "print('Weight Form of RNN layer : ',rnn_fin_layer.weights[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1t4MaeUy-Os",
        "outputId": "899e4542-27f9-4184-e872-f3fef079def1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence for RNN What time is it ?\n",
            "word mapping for Embedding :  [[2 3 0 1 4]]\n",
            "Form of input data :  (1, 5)\n",
            "\n",
            "Embedding Result :  (1, 5, 100)\n",
            "Weight Form of Embedding layer :  (5, 100)\n",
            "\n",
            "RNN Result :  (1, 5, 64)\n",
            "Weight Form of RNN layer :  (100, 64)\n",
            "\n",
            " RNN Result :  (1, 64)\n",
            "Weight Form of RNN layer :  (100, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To judge if the sentence is positive or negative,you can do it by reading entire sentence and check the output of last step.<br>\n",
        "But in case of generationg sentence and figure out positive or negative, you need outputs of every step.\n",
        " - in that case you can put `return_sequences=True` in `tf.keras.layers.SimpleRNN`"
      ],
      "metadata": {
        "id": "9pu0jmkwZoQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM\n",
        "Long Short-Term Memory<br>\n",
        "The Model is designed to avoid vanishing gradient problem.<br>\n",
        "\n",
        "- LSTM is basically RNN layer with 4 differnt weights.<br>\n",
        "- Each weights are included architecture called \"gate\" and determine which data should or should not affect to the next step.\n",
        "- Because LSTM has new concept called 'Cell state',it saves old input data without much loss.\n",
        "- cell states  add or substracts data that goes into gate.\n",
        "\n",
        "> GRU : Modified model of LSTM<br>\n",
        "- Forget Gate and Input Gate are combined in to Update Gate<br>\n",
        "- Cell state and Hidden state are also combined"
      ],
      "metadata": {
        "id": "d84SDLXsalaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_seq_layer = tf.keras.layers.LSTM(units=64, return_sequences=True, use_bias=False)\n",
        "lstm_seq_out = lstm_seq_layer(emb_out)\n",
        "\n",
        "print(\"\\nLSTM 결과 (모든 Step Output):\", lstm_seq_out.shape)\n",
        "print(\"LSTM Layer의 Weight 형태:\", lstm_seq_layer.weights[0].shape)\n",
        "\n",
        "lstm_fin_layer = tf.keras.layers.LSTM(units=64, use_bias=False)\n",
        "lstm_fin_out = lstm_fin_layer(emb_out)\n",
        "\n",
        "print(\"\\nLSTM 결과 (최종 Step Output):\", lstm_fin_out.shape)\n",
        "print(\"LSTM Layer의 Weight 형태:\", lstm_fin_layer.weights[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szi4yIS11jL-",
        "outputId": "eaad60c7-11fe-43ca-a355-51d2f23df591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "LSTM 결과 (모든 Step Output): (1, 5, 64)\n",
            "LSTM Layer의 Weight 형태: (100, 256)\n",
            "\n",
            "LSTM 결과 (최종 Step Output): (1, 64)\n",
            "LSTM Layer의 Weight 형태: (100, 256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bidrectional RNN\n",
        "- RNN that changed direction of progress.<br>\n",
        "- Two overlapping RNNs with opposite directions.<br>\n",
        "- Use `tf.keras.layers.Bidrectional()`<br>\n",
        "- Rather than analyzing or generating sentences, it's advantageous for tasks such as machine translation"
      ],
      "metadata": {
        "id": "0rKcxjoOcOto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "sentence = 'What time is it ?'\n",
        "dic = {\n",
        "    'is' : 0,\n",
        "    'it': 1,\n",
        "    'What' : 2,\n",
        "    'time' : 3,\n",
        "    '?' : 4,\n",
        "}\n",
        "\n",
        "sentence_tensor = tf.constant([[dic[word] for word in sentence.split()]])\n",
        "\n",
        "embedding_layer = tf.keras.layers.Embedding(input_dim=len(dic),output_dim=100)\n",
        "emb_out = embedding_layer(sentence_tensor)\n",
        "\n",
        "print('Form of input data :',emb_out.shape)\n",
        "\n",
        "bi_rnn = \\\n",
        "tf.keras.layers.Bidirectional(\n",
        "    tf.keras.layers.SimpleRNN(units=64,use_bias=False,return_sequences=True) \n",
        ")\n",
        "bi_out = bi_rnn(emb_out)\n",
        "\n",
        "print('Bidirectional RNN Result : ',bi_out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0SAX8iYfcrY",
        "outputId": "013a2202-42ff-48b9-e64e-f0e85963fe5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Form of input data : (1, 5, 100)\n",
            "Bidirectional RNN Result :  (1, 5, 128)\n"
          ]
        }
      ]
    }
  ]
}